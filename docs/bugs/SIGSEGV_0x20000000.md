# Bug: SIGSEGV at Fastmem Offset 0x20000000

## Status: OPEN

## Symptom

App crashes with SIGSEGV when running Black Ops. The crash always occurs at exactly:
```
fault_addr = fastmem_base + 0x20000000
```

Example from logs:
```
Fastmem base: 0x756f619000
Fault addr:   0x758f619000
Offset:       0x20000000 (exactly 512MB)
```

## Root Cause Analysis

### Why 0x20000000 is a problem

The fastmem region reserves 4GB of virtual address space but only maps the first 512MB:
- **Mapped:** 0x00000000 - 0x1FFFFFFF (accessible)
- **Unmapped:** 0x20000000+ (SIGSEGV on access)

### What address 0x20000000 means on Xbox 360

On the real Xbox 360, physical address 0x20000000 is a **mirror** of 0x00000000. The memory controller hardware performs:
```
actual_addr = requested_addr & 0x1FFFFFFF
```

Games can (and do) access these mirror addresses expecting them to work.

### Expected Behavior

The JIT should mask all fastmem-bound addresses:
```cpp
0x20000000 & 0x1FFFFFFF = 0x00000000  // Mirror maps to base
0x80000000 & 0x1FFFFFFF = 0x00000000  // Virtual maps to physical
0x9FFFFFFF & 0x1FFFFFFF = 0x1FFFFFFF  // Virtual maps to physical end
```

## Investigation History

### Attempt 1: Check for >= 0x20000000 → MMIO (WRONG)

Initial code routed addresses >= 0x20000000 to the MMIO slow path. This was incorrect because:
- 0x20000000-0x7FBFFFFF are memory mirrors, not MMIO
- Only 0x7FC00000-0x7FFFFFFF is GPU MMIO in the physical range

### Attempt 2: V4 Address Routing (IMPLEMENTED)

Changed logic to:
1. Check kernel (>= 0xA0000000) → MMIO
2. Check GPU virtual (0xC0000000-0xC3FFFFFF) → MMIO  
3. Check alt GPU virtual (0xEC800000-0xECFFFFFF) → MMIO
4. Check GPU MMIO physical (0x7FC00000-0x7FFFFFFF) → MMIO
5. **All other addresses:** Apply mask AND 0x1FFFFFFF → Fastmem

**Result:** Still crashing at exactly 0x20000000

## Current Hypothesis

The masking code IS present, but something is preventing it from being executed. Possibilities:

### H1: Branch Logic Bug
The conditional branches might be taking the wrong path for address 0x20000000.

For addr = 0x20000000:
- `CMP X0, 0xC0000000` → 0x20000000 < 0xC0000000 → CC is TRUE
- `B.CC below_gpu_virt` → branches (correct)
- `CMP X0, 0xA0000000` → 0x20000000 < 0xA0000000 → CS is FALSE  
- `B.CS kernel_space` → does NOT branch (correct)
- `CMP X0, 0x7FC00000` → 0x20000000 < 0x7FC00000 → CC is TRUE
- `B.CC below_gpu_phys` → branches (correct)
- Should reach mask... but doesn't?

### H2: Another Code Path
There might be another function accessing memory that we haven't patched.

Functions verified to have V4 routing:
- [x] compile_load
- [x] compile_store  
- [x] compile_load_multiple
- [x] compile_store_multiple
- [x] compile_atomic_load
- [x] compile_atomic_store
- [x] compile_dcbz

### H3: Interpreter Fallback
Some instructions might fall back to interpreter mode which uses different memory access code.

### H4: Non-JIT Memory Access
Something outside the JIT (kernel code, DMA, etc.) might be accessing fastmem directly.

### H5: Register Clobbering
X0 might be getting modified between address calculation and masking.

## Debug Steps To Try

### 1. Add Trace for 0x20000000 Range

Modify `compile_store` to log addresses in the 0x20000000-0x7FBFFFFF range:

```cpp
// Before address routing, add trace for mirror addresses
emit.MOV_imm(arm64::X16, 0x20000000ULL);
emit.CMP(arm64::X0, arm64::X16);
u8* skip_trace = emit.current();
emit.B_cond(arm64_cond::CC, 0);  // Skip if < 0x20000000

emit.MOV_imm(arm64::X16, 0x80000000ULL);
emit.CMP(arm64::X0, arm64::X16);
u8* in_mirror = emit.current();
emit.B_cond(arm64_cond::CC, 0);  // If < 0x80000000, it's a mirror

emit.patch_branch(skip_trace, emit.current());
// ... continue normal flow ...

// At in_mirror: call trace function
```

### 2. Verify Generated Code

Disassemble the block dump to verify mask instruction is present:
```
Look for: D29FFFF0 F2A3FFF0 8A100000
         MOV X16, #0xFFFF (low)
         MOVK X16, #0x1FFF, LSL#16 (high) 
         AND X0, X0, X16
```

### 3. Check for Early Returns

Search for any `return` statements in compile_load/compile_store before the routing logic.

### 4. Add Runtime Guard

In fastmem path, add a bounds check before the actual memory access:
```cpp
// After adding fastmem_base to X0:
emit.MOV_imm(arm64::X16, fastmem_base + 0x20000000);
emit.CMP(arm64::X0, arm64::X16);
// If >= limit, call error handler instead of crashing
```

## Files Modified

- `native/src/cpu/jit/jit_compiler.cpp`
  - compile_load (~line 1120-1290)
  - compile_store (~line 1363-1555)
  - compile_load_multiple (~line 1560-1620)
  - compile_store_multiple (~line 1627-1690)
  - compile_atomic_load (~line 1699-1760)
  - compile_atomic_store (~line 1760-1850)
  - compile_dcbz (~line 1853-1920)

## Related Documentation

- [JIT_COMPILER.md](JIT_COMPILER.md) - Full JIT architecture
- [JIT_FASTMEM_BUG.md](../bugs/JIT_FASTMEM_BUG.md) - Previous fastmem issues
